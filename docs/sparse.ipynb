{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4b1cde5",
   "metadata": {},
   "source": [
    "# Sparse Differentiation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de33c4d",
   "metadata": {},
   "source": [
    "\n",
    "## Problem\n",
    "\n",
    "In finite element analysis, the tangent stiffness matrix (Hessian) $\\mathbf{K}$ is typically **sparse**. A node only interacts with its immediate neighbors, meaning most entries in $\\mathbf{K}$ are zero. However, standard automatic differentiation (AD) in JAX (`jax.jacfwd` or `jax.jacrev`) is unaware of this sparsity. It attempts to recover the full dense matrix by evaluating the Jacobian-Vector Product (JVP) once for every degree of freedom.\n",
    "\n",
    "For a mesh with $N$ degrees of freedom:\n",
    "\n",
    "* **Naive AD Cost:** $N \\times t_{\\text{residual}}$ (Prohibitive for large $N$)\n",
    "* **Memory:** $O(N^2)$ (Explodes quickly)\n",
    "\n",
    "We need a way to compute **only the non-zero entries** of $\\mathbf{K}$ efficiently, ideally in constant time with respect to the mesh size.\n",
    "\n",
    "## Solution\n",
    "\n",
    "[`tatva.sparse`](api/tatva.sparse.md#sparse) provides a sparse differentiation engine that reduces the cost from $O(N)$ to $O(c)$, where $c$ is the \"chromatic number\" of the mesh (typically small and constant, e.g., ~10-20 for 2D meshes).\n",
    "\n",
    "The process has three steps:\n",
    "\n",
    "-  **Sparsity Pattern:** Identify the non-zero structure.\n",
    "-  **Coloring:** Group non-interacting DOFs.\n",
    "-  **Differentiation:** Compute the matrix in batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d62edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "jax.config.update(\"jax_enable_x64\", True)  # use double-precision\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import scipy.sparse as sp\n",
    "from tatva import sparse, Mesh\n",
    "\n",
    "mesh = Mesh.unit_square(n_x=1, n_y=1,  type=\"triangle\", dim=2)\n",
    "n_dofs_per_node = 2\n",
    "n_dofs = mesh.coords.shape[0] * n_dofs_per_node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be62ec74",
   "metadata": {},
   "source": [
    "\n",
    "### Sparsity Pattern\n",
    "\n",
    "First, we analyze the mesh connectivity to determine which DOFs interact. [`create_sparsity_pattern`](api/tatva.sparse.md#tatva.sparse.create_sparsity_pattern) returns the indices of the non-zero entries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "631489a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 56 non-zeros\n"
     ]
    }
   ],
   "source": [
    "# Extract sparsity topology from the mesh\n",
    "sparsity_pattern = sparse.create_sparsity_pattern(\n",
    "    mesh,\n",
    "    n_dofs_per_node=n_dofs_per_node\n",
    ")\n",
    "\n",
    "# Convert to Scipy CSR format for efficient indexing and later use\n",
    "sparsity_pattern_csr = sp.csr_matrix(\n",
    "    (\n",
    "        sparsity_pattern.data,\n",
    "        (sparsity_pattern.indices[:, 0], sparsity_pattern.indices[:, 1]),\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Sparsity: {sparsity_pattern_csr.nnz} non-zeros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d36c2e9",
   "metadata": {},
   "source": [
    "!!! info \"Sparsity Pattern\"\n",
    "\n",
    "    In `tatva` we provide a few functionalities to generate sparsity patetrn for some specific problem. \n",
    "    \n",
    "    - for a single physical field problem, [`sparse.create_sparsity_pattern`](api/tatva.sparse.md#tatva.sparse.create_sparsity_pattern)\n",
    "    - for KKT problems, [`sparse.create_sparsity_pattern_KKT`](api/tatva.sparse.md#tatva.sparse.create_sparsity_pattern_KKT)\n",
    "    - for reduced system via condensation  [`sparse.reduce_sparsity_pattern`](api/tatva.sparse.md#tatva.sparse.reduce_sparsity_pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f02412",
   "metadata": {},
   "source": [
    "### Graph Coloring\n",
    "\n",
    "We partition the degrees of freedom into independent sets (colors). Two DOFs share the same color **only if** they do not share an edge in the sparsity graph (Distance-1) and do not share a common neighbor (Distance-2). This ensures that when we perturb all DOFs of \"Color A\" simultaneously, their contributions to the Hessian do not overlap.\n",
    "\n",
    "!!! info \"Coloring Algorithm\"\n",
    "    \n",
    "    We use `tatva_coloring` library to generate colors from a sparsity pattern and take the first return value which is the colors. The implemented coloring algorithm is a naive greedy-algorithm. One can easily use other coloring libraries such as [!pysparsematrixcolorings](https://github.com/gdalle/pysparsematrixcolorings) to use advanced coloring algorithms which are efficient as they generate less number of colors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cc9f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of colors required: 8\n"
     ]
    }
   ],
   "source": [
    "from tatva_coloring import distance2_color_and_seeds\n",
    "\n",
    "colors = distance2_color_and_seeds(\n",
    "    row_ptr=sparsity_pattern_csr.indptr,\n",
    "    col_idx=sparsity_pattern_csr.indices,\n",
    "    n_dofs=n_dofs,\n",
    ")[0]\n",
    "\n",
    "print(f\"Number of colors required: {jnp.max(colors) + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab4cc5c",
   "metadata": {},
   "source": [
    "\n",
    "### Sparse Differentiation\n",
    "\n",
    "Finally, we use `jacfwd_with_batch`. This function automatically:\n",
    "-  Perturbs the input $\\boldsymbol{u}$ using the color groups.\n",
    "-  Evaluates the gradient efficiently (Batched JVPs).\n",
    "-  Reconstructs the values into the correct sparse matrix locations.\n",
    "\n",
    "\n",
    "!!! Info\n",
    "\n",
    "    We use our own implementation of sparse differentiation to make it scalable for large problems. But one can use libraries such as [!sparsejac](https://github.com/mfschubert/sparsejac) which has been an source of inspiration for our own implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a0a92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy_fn(u, delta):\n",
    "    # Placeholder for the actual energy function\n",
    "    return jnp.sum(u**2) + delta * jnp.sum(u)\n",
    "\n",
    "\n",
    "gradient_fn = jax.jacrev(energy_fn)\n",
    "\n",
    "# differentiate the residual using the sparsity information\n",
    "K_sparse_fn = sparse.jacfwd_with_batch(\n",
    "    gradient=gradient_fn,\n",
    "    row_ptr=jnp.array(sparsity_pattern_csr.indptr),\n",
    "    col_indices=jnp.array(sparsity_pattern_csr.indices),\n",
    "    colors=jnp.array(colors),\n",
    "    color_batch_size=mesh.elements.shape[0], # Batch size for evaluating the element routine\n",
    ")\n",
    "\n",
    "u_current = jnp.zeros(n_dofs)  # Example input\n",
    "delta_current = 1.0  # Example parameter\n",
    "K_sparse = K_sparse_fn(u_current, delta_current)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785d0173",
   "metadata": {},
   "source": [
    "!!! Info\n",
    "\n",
    "    The above function `K_sparse_fn` needs to be created only once given the sparsity pattern is not changing. Once created one can use te generated function within the simulation loop. \n",
    "    If the energy function takes additional arguments for example history parameters then the generated `K_sparse_fn` also takes the same arguments. \n",
    "\n",
    "\n",
    "!!! tip\n",
    "\n",
    "    Sometimes we need to evalues $\\mathbf{K}$ at fixed values for additinal arguments but updated values of $\\boldsymbol{u}$. For example, in Newton-Raphson or Staggered solvers. Use `equinox.Partial` from `equinox` to freeze some argument values. For example\n",
    "\n",
    "    ```python\n",
    "    import equinox as eqx\n",
    "\n",
    "    K_sparse_partial = eqx.Partial(K_sparse_fn, delta=delta_current)\n",
    "\n",
    "    # newton iteration\n",
    "    for i in range(iter):\n",
    "        ...\n",
    "        # call K_sparse_partial with just u\n",
    "        K_sparse = K_sparse_partial(u_new)\n",
    "        ...\n",
    "\n",
    "    ``` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25fd23f",
   "metadata": {},
   "source": [
    "## Matrix-Free Operators\n",
    "\n",
    "For extremely large problems, even storing the sparse matrix indices might be too memory-intensive. In these cases, we can use **Matrix-Free** methods. Since we have the energy functional, we can compute the Jacobian-vector product (JVP) $\\mathbf{K}\\mathbf{v}$ directly without ever forming $\\mathbf{K}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa7ccdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobian_vector_product(u, v):\n",
    "    \"\"\"\n",
    "    Computes (Hessian of Energy at u) * v. It is equivalent to: jvp( jacrev(energy)(u), v ).\n",
    "    Args:\n",
    "        u: Current solution (shape: [n_dofs])\n",
    "        v: Vector to multiply with the Hessian (shape: [n_dofs])\n",
    "    Returns:\n",
    "        The product of the Hessian of the energy function at u with the vector v (shape: [n_dofs])\n",
    "    \"\"\"\n",
    "    return jax.jvp(jax.jacrev(energy_fn), (u,), (v,))[1]\n",
    "\n",
    "# This can be passed directly to iterative solvers like jax.scipy.sparse.linalg.cg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tatva-examples",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
